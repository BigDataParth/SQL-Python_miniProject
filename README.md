# SQL-Python_miniProject

# Data Engineering Mini Project: Kaggle Dataset to MS SQL Server

## ğŸ“Œ Project Overview
This project demonstrates the end-to-end process of fetching a dataset from Kaggle, processing it using Pandas in Jupyter Notebook, and loading it into MS SQL Server for analysis. The project follows the following steps:

1. **Fetch dataset from Kaggle API**
2. **Load dataset into the system**
3. **Use Pandas to clean and process the CSV file in Jupyter Notebook**
4. **Load processed data into MS SQL Server using SQLAlchemy and pyodbc**
5. **Perform SQL queries to analyze and extract useful insights**

## ğŸ“‚ Project Structure
```
ğŸ“ Kaggle-to-SQL-Analysis
â”‚â”€â”€ ğŸ“„ README.md          # Project documentation
â”‚â”€â”€ ğŸ“„ main.ipynb     # Jupyter Notebook with data processing steps
â”‚â”€â”€ ğŸ“„ orders.csv        # Sample dataset (optional)
â”‚â”€â”€ ğŸ“„ sql_queries.sql    # SQL queries for analysis
```

## ğŸ› ï¸ Tools & Technologies Used
- **Python**
- **Jupyter Notebook**
- **Pandas**
- **Kaggle API**
- **SQLAlchemy & pyodbc** (for connecting to MS SQL Server)
- **Microsoft SQL Server** (for storing and analyzing data)

## ğŸ”§ Installation & Setup

### 1 Set Up Kaggle API
- Create an account on [Kaggle](https://www.kaggle.com/)
- Get your Kaggle API key from `Account Settings`
- Place `kaggle.json` in the `~/.kaggle/` directory

### 4ï¸âƒ£ Run Jupyter Notebook
```bash
jupyter notebook
```
Open `notebook.ipynb` and execute the cells step by step.

### 5ï¸âƒ£ Set Up MS SQL Server Connection
Modify the connection string in the notebook to match your SQL Server details:
```python
from sqlalchemy import create_engine
import pyodbc

server = 'PARTH\SQLEXPRESS'
database = 'master'
username = 'parth'
password = 'your_password'

driver = '{ODBC Driver 17 for SQL Server}'
engine = create_engine(f'mssql+pyodbc://{username}:{password}@{server}/{database}?driver={driver}')
```

### 6ï¸âƒ£ Load Data into SQL Server
Run the script in the notebook to write the DataFrame to SQL Server:
```python
df.to_sql('df_orders', con=engine, if_exists='replace', index=False)
```

## ğŸ“Š SQL Analysis
After loading the data into MS SQL Server, execute the queries in `sql_queries.sql` to analyze the dataset. Some example queries include:
-- For each category ,highest sale month

with cte as (
select category,format(order_date,'yyyyMM') as order_year_month
, sum(sale_pricee) as sales 
from df_orders
group by category,format(order_date,'yyyyMM')
--order by category,format(order_date,'yyyyMM')
)
select * from (
select *,
row_number() over(partition by category order by sales desc) as rn
from cte
) a
where rn=1


## ğŸš€ Outcomes & Insights
- Successfully fetched and processed data from Kaggle
- Cleaned and transformed the dataset using Pandas
- Stored data in MS SQL Server using SQLAlchemy and pyodbc
- Performed SQL queries to extract insights from the data

## ğŸ“¢ Contributing
Feel free to raise issues or contribute by submitting pull requests!

## ğŸ† Author
- **Your Name**
- **LinkedIn:** [Parth Bhavthankar](www.linkedin.com/in/parthbhavthankar)
- **GitHub:** [BigDataParth](https://github.com/BigDataParth)

---
### â­ If you found this project useful, don't forget to give it a star! â­

